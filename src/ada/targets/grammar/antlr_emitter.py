#!/usr/bin/env python3
"""ANTLR grammar emitter.

Emits grammars in ANTLR4 format:
- Grammar declaration: grammar Name;
- Parser rules (lowercase): rule : alternative1 | alternative2 ;
- Lexer rules (UPPERCASE): TOKEN : pattern ;
- Fragments: fragment DIGIT : [0-9] ;
"""

from typing import Dict, List, Optional, Any, Set

from ir.grammar.grammar_ir import Grammar, EmitterResult
from ir.grammar.production import ProductionRule, BodyElement
from ir.grammar.production import OptionalOp, Repetition, OneOrMore, Group, Alternation
from ir.grammar.symbol import Symbol
from targets.grammar.base import GrammarEmitterBase


class ANTLREmitter(GrammarEmitterBase):
    """Emitter for ANTLR4 grammar format.
    
    ANTLR format:
        grammar Calculator;
        
        expr : term (('+' | '-') term)* ;
        term : factor (('*' | '/') factor)* ;
        factor : NUM | '(' expr ')' ;
        
        NUM : [0-9]+ ;
        WS : [ \\t\\r\\n]+ -> skip ;
    
    Config options:
        grammar_type (str): "parser", "lexer", or "combined" (default: "combined")
        generate_lexer_rules (bool): Auto-generate lexer rules for terminals (default: True)
        skip_whitespace (bool): Add whitespace skip rule (default: True)
    """
    
    FORMAT = "antlr"
    FILE_EXTENSION = ".g4"
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self.grammar_type = self.config.get('grammar_type', 'combined')
        self.generate_lexer_rules = self.config.get('generate_lexer_rules', True)
        self.skip_whitespace = self.config.get('skip_whitespace', True)
    
    def emit(self, grammar: Grammar) -> EmitterResult:
        """Emit grammar in ANTLR format.
        
        Args:
            grammar: The grammar to emit
        
        Returns:
            EmitterResult with ANTLR code and manifest
        """
        self._warnings = []
        lines = []
        
        # Grammar declaration
        antlr_name = self._to_antlr_name(grammar.name)
        
        if self.grammar_type == 'parser':
            lines.append(f"parser grammar {antlr_name};")
            lines.append("")
            lines.append(f"options {{ tokenVocab = {antlr_name}Lexer; }}")
        elif self.grammar_type == 'lexer':
            lines.append(f"lexer grammar {antlr_name};")
        else:
            lines.append(f"grammar {antlr_name};")
        
        lines.append("")
        
        # Add header comment
        lines.append("// Generated by STUNIR Grammar Emitter")
        lines.append(f"// Source: {grammar.name}")
        lines.append("")
        
        # Collect terminals that need lexer rules
        terminals_needing_rules: Set[Symbol] = set()
        
        # Emit parser rules (non-terminals)
        if self.grammar_type != 'lexer':
            lines.append("// Parser Rules")
            lines.append("")
            
            for nt in sorted(grammar.nonterminals, key=lambda s: s.name):
                prods = grammar.get_productions(nt)
                if not prods:
                    continue
                
                # Track terminals in this rule
                for prod in prods:
                    for sym in prod.body_symbols():
                        if sym.is_terminal():
                            terminals_needing_rules.add(sym)
                
                # Emit rule
                rule_name = self._to_parser_rule_name(nt.name)
                alternatives = [self._emit_body(p) for p in prods]
                
                if len(alternatives) == 1:
                    lines.append(f"{rule_name}")
                    lines.append(f"    : {alternatives[0]}")
                    lines.append(f"    ;")
                else:
                    lines.append(f"{rule_name}")
                    lines.append(f"    : {alternatives[0]}")
                    for alt in alternatives[1:]:
                        lines.append(f"    | {alt}")
                    lines.append(f"    ;")
                lines.append("")
        
        # Emit lexer rules (terminals)
        if self.grammar_type != 'parser' and self.generate_lexer_rules:
            lines.append("// Lexer Rules")
            lines.append("")
            
            for term in sorted(terminals_needing_rules, key=lambda s: s.name):
                token_name = self._to_token_name(term.name)
                if term.pattern:
                    # Use provided pattern
                    lines.append(f"{token_name} : {term.pattern} ;")
                else:
                    # Generate simple literal match
                    escaped = self._escape_antlr_literal(term.name)
                    lines.append(f"{token_name} : '{escaped}' ;")
            
            lines.append("")
            
            # Add whitespace skip rule
            if self.skip_whitespace:
                lines.append("// Skip whitespace")
                lines.append("WS : [ \\t\\r\\n]+ -> skip ;")
                lines.append("")
        
        code = "\n".join(lines)
        manifest = self._generate_manifest(grammar, code)
        
        return EmitterResult(
            code=code,
            manifest=manifest,
            format=self.FORMAT,
            warnings=self._warnings
        )
    
    def emit_production(self, rule: ProductionRule) -> str:
        """Emit a single production rule in ANTLR format.
        
        Args:
            rule: The production rule to emit
        
        Returns:
            ANTLR production string
        """
        rule_name = self._to_parser_rule_name(rule.head.name)
        body = self._emit_body(rule)
        return f"{rule_name} : {body} ;"
    
    def _emit_body(self, rule: ProductionRule) -> str:
        """Emit the body of a production rule."""
        if rule.is_epsilon_production():
            return "/* empty */"
        
        parts = []
        for elem in rule.body:
            parts.append(self._emit_antlr_element(elem))
        
        return " ".join(parts)
    
    def _emit_antlr_element(self, element: BodyElement) -> str:
        """Emit a body element with ANTLR notation."""
        if isinstance(element, Symbol):
            if element.is_nonterminal():
                return self._to_parser_rule_name(element.name)
            elif element.is_terminal():
                return self._to_token_name(element.name)
            elif element.is_epsilon():
                return "/* empty */"
            else:
                return str(element)
        elif isinstance(element, OptionalOp):
            inner = self._emit_antlr_element(element.element)
            return f"{inner}?"
        elif isinstance(element, Repetition):
            inner = self._emit_antlr_element(element.element)
            return f"{inner}*"
        elif isinstance(element, OneOrMore):
            inner = self._emit_antlr_element(element.element)
            return f"{inner}+"
        elif isinstance(element, Group):
            inner = " ".join(self._emit_antlr_element(e) for e in element.elements)
            return f"({inner})"
        elif isinstance(element, Alternation):
            parts = []
            for alt in element.alternatives:
                if isinstance(alt, tuple):
                    parts.append(" ".join(self._emit_antlr_element(e) for e in alt))
                else:
                    parts.append(self._emit_antlr_element(alt))
            return f"({' | '.join(parts)})"
        else:
            return str(element)
    
    def _to_antlr_name(self, name: str) -> str:
        """Convert name to valid ANTLR grammar name (PascalCase)."""
        # Remove invalid chars and convert to PascalCase
        words = name.replace('-', '_').replace('.', '_').split('_')
        return ''.join(w.capitalize() for w in words)
    
    def _to_parser_rule_name(self, name: str) -> str:
        """Convert to ANTLR parser rule name (lowercase)."""
        # Parser rules must start with lowercase
        clean = name.replace('-', '_').replace('.', '_')
        return clean.lower()
    
    def _to_token_name(self, name: str) -> str:
        """Convert to ANTLR token/lexer rule name (UPPERCASE)."""
        # Lexer rules must start with uppercase
        clean = name.replace('-', '_').replace('.', '_')
        if clean[0].isdigit():
            clean = 'T_' + clean
        return clean.upper()
    
    def _escape_antlr_literal(self, s: str) -> str:
        """Escape string for ANTLR literal."""
        return s.replace("'", "\\'")
    
    def _get_comment_char(self) -> str:
        return "//"
