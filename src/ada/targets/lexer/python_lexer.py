"""
Python Lexer Emitter for STUNIR.

Generates Python lexer code from DFA specification.
"""

from typing import List

from ir.lexer.token_spec import LexerSpec, TokenSpec
from ir.lexer.dfa import MinimizedDFA, TransitionTable
from .base import BaseLexerEmitter, EmitterUtils


class PythonLexerEmitter(BaseLexerEmitter):
    """
    Emit Python lexer code.
    
    Generates a self-contained Python module with:
    - Token dataclass
    - TokenType constants
    - DFA transition tables
    - Lexer class with tokenize() method
    """
    
    def emit(self, spec: LexerSpec, dfa: MinimizedDFA, table: TransitionTable) -> str:
        """Emit complete Python lexer module."""
        lines = [
            '"""',
            f'Generated Lexer: {spec.name}',
            f'Tokens: {len(spec.tokens)}',
            f'DFA States: {dfa.num_states}',
            f'Alphabet Size: {len(dfa.alphabet)}',
            '',
            'Generated by STUNIR Lexer Generator',
            '"""',
            '',
            'from dataclasses import dataclass',
            'from typing import List, Optional, Tuple',
            '',
            '',
            self.emit_token_class(spec),
            '',
            '',
            self.emit_transition_table(table),
            '',
            '',
            self.emit_lexer_class(spec, dfa),
        ]
        return '\n'.join(lines)
    
    def emit_token_class(self, spec: LexerSpec) -> str:
        """Emit Token dataclass and TokenType constants."""
        lines = [
            '@dataclass',
            'class Token:',
            f'{self.indent}"""Token produced by lexer."""',
            f'{self.indent}type: str',
            f'{self.indent}value: str',
            f'{self.indent}line: int',
            f'{self.indent}column: int',
            '',
            f'{self.indent}def __repr__(self) -> str:',
            f'{self.indent}{self.indent}return f"Token({{self.type}}, {{self.value!r}}, {{self.line}}:{{self.column}})"',
            '',
            '',
            'class TokenType:',
            f'{self.indent}"""Token type constants."""',
        ]
        
        for token in spec.tokens:
            lines.append(f'{self.indent}{token.name} = "{token.name}"')
        
        return '\n'.join(lines)
    
    def emit_transition_table(self, table: TransitionTable) -> str:
        """Emit DFA transition table."""
        lines = [
            '# DFA Transition Table',
            f'_START_STATE = {table.start_state}',
            f'_ERROR_STATE = {table.error_state}',
            f'_NUM_STATES = {table.num_states}',
            f'_NUM_SYMBOLS = {table.num_symbols}',
            '',
            '# Symbol to index mapping',
        ]
        
        # Format symbol map
        symbol_entries = []
        for symbol, idx in sorted(table.symbol_to_index.items(), key=lambda x: x[1]):
            escaped = self._escape_string(symbol, "'")
            symbol_entries.append(f"'{escaped}': {idx}")
        
        lines.append('_SYMBOL_MAP = {')
        # Group entries for readability
        for i in range(0, len(symbol_entries), 8):
            chunk = symbol_entries[i:i+8]
            lines.append(f'{self.indent}{", ".join(chunk)},')
        lines.append('}')
        lines.append('')
        
        # Format transition table
        lines.append('# Transition table [state * num_symbols + symbol_idx] -> next_state')
        lines.append('_TRANSITIONS = [')
        for i in range(0, len(table.table), 16):
            chunk = table.table[i:i+16]
            lines.append(f'{self.indent}{", ".join(str(v) for v in chunk)},')
        lines.append(']')
        lines.append('')
        
        # Format accept table
        lines.append('# Accept table: state -> (token_name, priority) or None')
        lines.append('_ACCEPT = [')
        for state_idx, accept_info in enumerate(table.accept_table):
            if accept_info:
                token_name, priority = accept_info
                lines.append(f'{self.indent}("{token_name}", {priority}),  # state {state_idx}')
            else:
                lines.append(f'{self.indent}None,  # state {state_idx}')
        lines.append(']')
        
        return '\n'.join(lines)
    
    def emit_lexer_class(self, spec: LexerSpec, dfa: MinimizedDFA) -> str:
        """Emit lexer class."""
        skip_tokens = {t.name for t in spec.tokens if t.skip}
        skip_set_str = '{' + ', '.join(f'"{t}"' for t in sorted(skip_tokens)) + '}'
        
        return f'''class LexerError(Exception):
    """Exception raised for lexer errors."""
    
    def __init__(self, message: str, line: int = 0, column: int = 0):
        super().__init__(message)
        self.line = line
        self.column = column
    
    def __str__(self) -> str:
        if self.line and self.column:
            return f"{{self.args[0]}} at line {{self.line}}, column {{self.column}}"
        return self.args[0]


class {spec.name}Lexer:
    """DFA-based lexer for {spec.name}."""
    
    SKIP_TOKENS = {skip_set_str}
    
    def __init__(self, input_str: str):
        """Initialize lexer with input string."""
        self.input = input_str
        self.pos = 0
        self.line = 1
        self.column = 1
    
    def tokenize(self) -> List[Token]:
        """Tokenize the entire input."""
        tokens = []
        while self.pos < len(self.input):
            token = self._next_token()
            if token and token.type not in self.SKIP_TOKENS:
                tokens.append(token)
        return tokens
    
    def tokenize_all(self) -> List[Token]:
        """Tokenize including skip tokens (useful for syntax highlighting)."""
        tokens = []
        while self.pos < len(self.input):
            token = self._next_token()
            if token:
                tokens.append(token)
        return tokens
    
    def _next_token(self) -> Optional[Token]:
        """Get next token using longest match."""
        if self.pos >= len(self.input):
            return None
        
        state = _START_STATE
        last_accept_pos = -1
        last_accept_info = None
        current_pos = self.pos
        
        while current_pos < len(self.input):
            char = self.input[current_pos]
            idx = _SYMBOL_MAP.get(char, -1)
            
            if idx < 0 or state < 0:
                break
            
            next_state = _TRANSITIONS[state * _NUM_SYMBOLS + idx]
            if next_state == _ERROR_STATE:
                break
            
            state = next_state
            current_pos += 1
            
            if state >= 0 and _ACCEPT[state] is not None:
                last_accept_pos = current_pos
                last_accept_info = _ACCEPT[state]
        
        if last_accept_pos < 0:
            raise LexerError(
                f"Unexpected character '{{self.input[self.pos]}}'",
                line=self.line,
                column=self.column
            )
        
        lexeme = self.input[self.pos:last_accept_pos]
        token_name = last_accept_info[0]
        
        token = Token(
            type=token_name,
            value=lexeme,
            line=self.line,
            column=self.column
        )
        
        # Update position
        for c in lexeme:
            if c == '\\n':
                self.line += 1
                self.column = 1
            else:
                self.column += 1
        self.pos = last_accept_pos
        
        return token
    
    def reset(self) -> None:
        """Reset lexer to beginning of input."""
        self.pos = 0
        self.line = 1
        self.column = 1


def tokenize(input_str: str) -> List[Token]:
    """Convenience function to tokenize a string."""
    lexer = {spec.name}Lexer(input_str)
    return lexer.tokenize()
'''
