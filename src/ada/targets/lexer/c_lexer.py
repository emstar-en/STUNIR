"""
C Lexer Emitter for STUNIR.

Generates C lexer code from DFA specification.
Produces both header (.h) and source (.c) files.
"""

from typing import List

from ir.lexer.token_spec import LexerSpec, TokenSpec
from ir.lexer.dfa import MinimizedDFA, TransitionTable
from .base import BaseLexerEmitter, EmitterUtils


class CLexerEmitter(BaseLexerEmitter):
    """
    Emit C lexer code.
    
    Generates a C implementation with:
    - Header file with Token struct and API
    - Source file with DFA tables and lexer implementation
    """
    
    def emit(self, spec: LexerSpec, dfa: MinimizedDFA, table: TransitionTable) -> str:
        """Emit complete C lexer (header + source)."""
        header = self._emit_header(spec, dfa, table)
        source = self._emit_source(spec, dfa, table)
        
        return f'''/* ============================================================
 * HEADER FILE: {spec.name.lower()}_lexer.h
 * ============================================================ */

{header}

/* ============================================================
 * SOURCE FILE: {spec.name.lower()}_lexer.c
 * ============================================================ */

{source}'''
    
    def _emit_header(self, spec: LexerSpec, dfa: MinimizedDFA, table: TransitionTable) -> str:
        """Emit header file."""
        guard = f'{spec.name.upper()}_LEXER_H'
        
        lines = [
            f'#ifndef {guard}',
            f'#define {guard}',
            '',
            '/*',
            f' * Generated Lexer: {spec.name}',
            f' * Tokens: {len(spec.tokens)}',
            f' * DFA States: {dfa.num_states}',
            ' *',
            ' * Generated by STUNIR Lexer Generator',
            ' */',
            '',
            '#include <stddef.h>',
            '#include <stdint.h>',
            '',
            '/* Token types */',
            'typedef enum {',
        ]
        
        for i, token in enumerate(spec.tokens):
            lines.append(f'    TOKEN_{token.name} = {i},')
        
        lines.extend([
            f'    TOKEN_EOF = {len(spec.tokens)},',
            f'    TOKEN_ERROR = {len(spec.tokens) + 1}',
            f'}} {spec.name}TokenType;',
            '',
            '/* Token structure */',
            f'typedef struct {{',
            f'    {spec.name}TokenType type;',
            '    const char* value;',
            '    size_t length;',
            '    size_t line;',
            '    size_t column;',
            f'}} {spec.name}Token;',
            '',
            '/* Lexer structure */',
            f'typedef struct {{',
            '    const char* input;',
            '    size_t input_len;',
            '    size_t pos;',
            '    size_t line;',
            '    size_t column;',
            f'}} {spec.name}Lexer;',
            '',
            '/* API functions */',
            f'void {spec.name.lower()}_lexer_init({spec.name}Lexer* lexer, const char* input, size_t len);',
            f'{spec.name}Token {spec.name.lower()}_lexer_next_token({spec.name}Lexer* lexer);',
            f'int {spec.name.lower()}_lexer_is_skip_token({spec.name}TokenType type);',
            f'const char* {spec.name.lower()}_token_type_name({spec.name}TokenType type);',
            '',
            f'#endif /* {guard} */',
        ])
        
        return '\n'.join(lines)
    
    def _emit_source(self, spec: LexerSpec, dfa: MinimizedDFA, table: TransitionTable) -> str:
        """Emit source file."""
        lines = [
            f'#include "{spec.name.lower()}_lexer.h"',
            '#include <string.h>',
            '',
            '/*',
            f' * Generated Lexer Implementation: {spec.name}',
            ' * Generated by STUNIR Lexer Generator',
            ' */',
            '',
            '/* DFA Constants */',
            f'#define START_STATE {table.start_state}',
            f'#define ERROR_STATE {table.error_state}',
            f'#define NUM_STATES {table.num_states}',
            f'#define NUM_SYMBOLS {table.num_symbols}',
            '',
        ]
        
        # Symbol to index mapping
        lines.append('/* Symbol to index mapping */')
        lines.append('static int symbol_to_index(char c) {')
        lines.append('    switch ((unsigned char)c) {')
        
        for symbol, idx in sorted(table.symbol_to_index.items(), key=lambda x: x[1]):
            escaped = self._escape_c_char(symbol)
            lines.append(f"        case '{escaped}': return {idx};")
        
        lines.append('        default: return -1;')
        lines.append('    }')
        lines.append('}')
        lines.append('')
        
        # Transition table
        lines.append('/* DFA transition table */')
        lines.append('static const int32_t transitions[] = {')
        for i in range(0, len(table.table), 16):
            chunk = table.table[i:i+16]
            lines.append(f'    {", ".join(str(v) for v in chunk)},')
        lines.append('};')
        lines.append('')
        
        # Accept table
        lines.append('/* Accept table: token type for each accepting state (-1 = not accepting) */')
        lines.append('static const int32_t accept_token[] = {')
        for state_idx, accept_info in enumerate(table.accept_table):
            if accept_info:
                token_name, priority = accept_info
                token_idx = next(i for i, t in enumerate(spec.tokens) if t.name == token_name)
                lines.append(f'    {token_idx},  /* state {state_idx}: {token_name} */')
            else:
                lines.append(f'    -1,  /* state {state_idx} */')
        lines.append('};')
        lines.append('')
        
        # Skip tokens
        skip_tokens = [t.name for t in spec.tokens if t.skip]
        lines.append('/* Skip token check */')
        lines.append(f'int {spec.name.lower()}_lexer_is_skip_token({spec.name}TokenType type) {{')
        lines.append('    switch (type) {')
        for token in skip_tokens:
            lines.append(f'        case TOKEN_{token}: return 1;')
        lines.append('        default: return 0;')
        lines.append('    }')
        lines.append('}')
        lines.append('')
        
        # Token type names
        lines.append('/* Token type names */')
        lines.append(f'const char* {spec.name.lower()}_token_type_name({spec.name}TokenType type) {{')
        lines.append('    switch (type) {')
        for token in spec.tokens:
            lines.append(f'        case TOKEN_{token.name}: return "{token.name}";')
        lines.append('        case TOKEN_EOF: return "EOF";')
        lines.append('        case TOKEN_ERROR: return "ERROR";')
        lines.append('        default: return "UNKNOWN";')
        lines.append('    }')
        lines.append('}')
        lines.append('')
        
        # Lexer implementation
        lines.append('/* Initialize lexer */')
        lines.append(f'void {spec.name.lower()}_lexer_init({spec.name}Lexer* lexer, const char* input, size_t len) {{')
        lines.append('    lexer->input = input;')
        lines.append('    lexer->input_len = len;')
        lines.append('    lexer->pos = 0;')
        lines.append('    lexer->line = 1;')
        lines.append('    lexer->column = 1;')
        lines.append('}')
        lines.append('')
        
        lines.append('/* Get next token */')
        lines.append(f'{spec.name}Token {spec.name.lower()}_lexer_next_token({spec.name}Lexer* lexer) {{')
        lines.append(f'    {spec.name}Token token;')
        lines.append('    token.line = lexer->line;')
        lines.append('    token.column = lexer->column;')
        lines.append('')
        lines.append('    if (lexer->pos >= lexer->input_len) {')
        lines.append('        token.type = TOKEN_EOF;')
        lines.append('        token.value = "";')
        lines.append('        token.length = 0;')
        lines.append('        return token;')
        lines.append('    }')
        lines.append('')
        lines.append('    int32_t state = START_STATE;')
        lines.append('    size_t last_accept_pos = (size_t)-1;')
        lines.append('    int32_t last_accept_token = -1;')
        lines.append('    size_t current_pos = lexer->pos;')
        lines.append('')
        lines.append('    while (current_pos < lexer->input_len) {')
        lines.append('        char c = lexer->input[current_pos];')
        lines.append('        int idx = symbol_to_index(c);')
        lines.append('')
        lines.append('        if (idx < 0 || state < 0) break;')
        lines.append('')
        lines.append('        int32_t next_state = transitions[state * NUM_SYMBOLS + idx];')
        lines.append('        if (next_state == ERROR_STATE) break;')
        lines.append('')
        lines.append('        state = next_state;')
        lines.append('        current_pos++;')
        lines.append('')
        lines.append('        if (state >= 0 && accept_token[state] >= 0) {')
        lines.append('            last_accept_pos = current_pos;')
        lines.append('            last_accept_token = accept_token[state];')
        lines.append('        }')
        lines.append('    }')
        lines.append('')
        lines.append('    if (last_accept_pos == (size_t)-1) {')
        lines.append('        token.type = TOKEN_ERROR;')
        lines.append('        token.value = lexer->input + lexer->pos;')
        lines.append('        token.length = 1;')
        lines.append('        lexer->pos++;')
        lines.append('        lexer->column++;')
        lines.append('        return token;')
        lines.append('    }')
        lines.append('')
        lines.append(f'    token.type = ({spec.name}TokenType)last_accept_token;')
        lines.append('    token.value = lexer->input + lexer->pos;')
        lines.append('    token.length = last_accept_pos - lexer->pos;')
        lines.append('')
        lines.append('    /* Update position */')
        lines.append('    for (size_t i = lexer->pos; i < last_accept_pos; i++) {')
        lines.append("        if (lexer->input[i] == '\\n') {")
        lines.append('            lexer->line++;')
        lines.append('            lexer->column = 1;')
        lines.append('        } else {')
        lines.append('            lexer->column++;')
        lines.append('        }')
        lines.append('    }')
        lines.append('    lexer->pos = last_accept_pos;')
        lines.append('')
        lines.append('    return token;')
        lines.append('}')
        
        return '\n'.join(lines)
    
    def emit_transition_table(self, table: TransitionTable) -> str:
        """Emit transition table (included in source)."""
        return ""  # Handled in _emit_source
    
    def emit_token_class(self, spec: LexerSpec) -> str:
        """Emit token struct (included in header)."""
        return ""  # Handled in _emit_header
    
    def emit_lexer_class(self, spec: LexerSpec, dfa: MinimizedDFA) -> str:
        """Emit lexer implementation (included in source)."""
        return ""  # Handled in _emit_source
    
    def _escape_c_char(self, c: str) -> str:
        """Escape character for C char literal."""
        if c == '\\':
            return '\\\\'
        elif c == "'":
            return "\\'"
        elif c == '\n':
            return '\\n'
        elif c == '\r':
            return '\\r'
        elif c == '\t':
            return '\\t'
        elif c == '\0':
            return '\\0'
        elif ord(c) < 32 or ord(c) > 126:
            return f'\\x{ord(c):02x}'
        return c
