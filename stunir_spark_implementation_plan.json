{
  "project": "STUNIR SPARK Implementation Plan",
  "version": "1.0.0",
  "created": "2026-02-05",
  "objective": "Achieve DO-333 compliance through SPARK implementation and fix all code generation issues",

  "scope": {
    "in_scope": [
      "SPARK implementations for Spec_Assembler, IR_Converter, Code_Emitter, Pipeline_Driver",
      "Unified extraction schema and multi-format parsing",
      "Type mapping and target code generation for 10 languages",
      "Formal verification artifacts and evidence package",
      "End-to-end pipeline tests for gnu_bc, lua_5.4, sqlite"
    ],
    "out_of_scope": [
      "Semantic correctness of generated implementations",
      "Optimization of generated code",
      "Source extraction tooling modifications"
    ]
  },

  "constraints": {
    "language_priority": "All implementations, solutions, and embodiments must be in Ada SPARK",
    "no_runtime_exceptions": true,
    "deterministic_output": true,
    "windows_compatibility": true
  },

  "assumptions": [
    "GNAT/SPARK toolchain will be available for CI and verification",
    "Extraction JSON files are valid JSON",
    "Existing tests in test_stunir_pipeline.py remain authoritative"
  ],

  "non_goals": [
    "Refactor existing Python bridge scripts beyond compatibility",
    "Change input extraction format at the source",
    "Introduce new target languages"
  ],

  "glossary": {
    "Spec": "Normalized specification JSON",
    "IR": "Intermediate Representation JSON",
    "Extraction": "Source extraction JSON from native tooling",
    "Phase": "Pipeline stage: Spec -> IR -> Code"
  },

  "phases": [
    {
      "phase": 1,
      "name": "Fix Code Generation Issues",
      "priority": "Immediate",
      "description": "Address all compilation errors in generated target language code",
      "tasks": [
        {
          "id": "1.1",
          "name": "Type Mapping System",
          "status": "pending",
          "description": "Create comprehensive type registry for C-to-target mappings",
          "issues_addressed": [
            "Undefined custom types (program_counter, etc.)",
            "Pointer type handling across languages",
            "Struct and typedef mappings"
          ],
          "inputs": ["IR type strings", "target language"],
          "outputs": ["target type string", "required forward declarations"],
          "acceptance_criteria": [
            "All pointer types map to valid target syntax",
            "Custom structs produce deterministic placeholder types",
            "No target language output contains raw '*' tokens unless valid in that language"
          ],
          "implementation": {
            "file": "tools/spark/src/core/type_mapper.ads",
            "approach": "SPARK package with pre/post conditions for type safety",
            "mappings": {
              "c_to_cpp": {
                "program_counter": "struct program_counter",
                "char*": "std::string",
                "int": "int",
                "void": "void"
              },
              "c_to_rust": {
                "program_counter": "ProgramCounter",
                "program_counter*": "*mut ProgramCounter",
                "char*": "String",
                "int": "i32",
                "void": "()"
              },
              "c_to_python": {
                "program_counter": "Any",
                "program_counter*": "Any",
                "char*": "str",
                "int": "int",
                "void": "None"
              },
              "c_to_go": {
                "program_counter": "ProgramCounter",
                "program_counter*": "*ProgramCounter",
                "char*": "string",
                "int": "int",
                "void": ""
              }
            }
          }
        },
        {
          "id": "1.2",
          "name": "Rust Parameter Syntax Fix",
          "status": "pending",
          "description": "Fix parameter order from C-style to Rust-style",
          "issue": "Current output: pub fn stop_execution(i32 sig)",
          "fix": "Correct output: pub fn stop_execution(sig: i32)",
          "inputs": ["parameter name", "parameter type"],
          "outputs": ["rust parameter string"],
          "acceptance_criteria": [
            "All generated Rust signatures compile",
            "No parameter uses C-style ordering"
          ],
          "implementation": {
            "file": "tools/spark/src/core/code_emitter.adb",
            "function": "Generate_Rust_Parameter",
            "contracts": {
              "pre": "Parameter_Name'Length > 0 and Type_Name'Length > 0",
              "post": "Result contains Name & ': ' & Type"
            }
          }
        },
        {
          "id": "1.3",
          "name": "Python Type Hint Fix",
          "status": "pending",
          "description": "Remove C pointer syntax from Python type hints",
          "issue": "Current output: progctr: program_counter *",
          "fix": "Correct output: progctr: Any",
          "inputs": ["IR type string"],
          "outputs": ["python type annotation"],
          "acceptance_criteria": [
            "All generated Python files pass py_compile",
            "No raw '*' tokens appear in type annotations"
          ],
          "implementation": {
            "file": "tools/spark/src/core/code_emitter.adb",
            "function": "Generate_Python_Type_Annotation",
            "approach": "Strip '*' suffix, map to Python typing module types"
          }
        },
        {
          "id": "1.4",
          "name": "Return Statement Generation",
          "status": "pending",
          "description": "Generate proper return statements for non-void functions",
          "default_values": {
            "int": "0",
            "i32": "0",
            "bool": "false",
            "char": "'\\0'",
            "char*": "None",
            "String": "String::new()",
            "str": "\"\"",
            "string": "\"\"",
            "pointer_types": "null",
            "void": ""
          },
          "inputs": ["return type", "target language"],
          "outputs": ["return statement string"],
          "acceptance_criteria": [
            "No compilation warnings for missing returns",
            "Return statements are valid for each target language"
          ],
          "implementation": {
            "file": "tools/spark/src/core/code_emitter.adb",
            "function": "Generate_Return_Statement",
            "contracts": {
              "pre": "Return_Type is valid",
              "post": "Result is syntactically correct for target language"
            }
          }
        }
      ]
    },
    {
      "phase": 2,
      "name": "SPARK Component Implementation",
      "priority": "High",
      "description": "Replace Python bridges with formally verified SPARK components",
      "do_333_compliance": {
        "objective": "Achieve Gold/Platinum SPARK verification level",
        "requirements": [
          "No runtime errors (AOR, COR, DOR, FOV, POR, ROR, SIO, UOE)",
          "Memory safety (ALI, NAF, NDL, NFI, NII, NTL, OBL, OBA, OBS, OFL, OFN, OTF, SAW, UAF, UMR)",
          "Flow analysis (CAF, CFC, DAA, DCA, DCF, DFE, DOS, DSA, DSB, DSC, DSD, DSF, DTI, KCU, NCA, NCF, NDE, NDS, NDT, NFI, NFW, NOD, NRB, NRC, NTD, NTF, NTK, NTR, NUA, NUN, NUU, OAT, OCU, ODK, ODT, OFL, ORA, ORM, ORN, ORP, ORU, OTF, OUO, OUU, PBU, PCP, PCV, PEA, PEE, PEU, PIA, PII, PIN, PLM, PNA, PNR, PPV, PRN, PUN, RCA, RCF, RCK, RNC, RNV, RSF, RSN, RSP, RST, RTA, RTC, RTF, RUQ, RUS, RUW, SAW, SIB, SIO, SRM, SSD, STP, STR, SUB, TCF, TCO, TOV, UAO, UAT, UCE, UFM, UMR, UNU, UOE, UOO, UOR, UOT, UOU, URA, URM, URN, URP, URU, UTF, UTN, UUO, WAW, WDW, WEW, WFW, WIG, WIS, WIW, WRA, WRW, WSI, WST, WWI, WWU, ZZZ)"
        ]
      },
      "tasks": [
        {
          "id": "2.1",
          "name": "SPARK Spec Assembler",
          "status": "pending",
          "description": "Replace Python spec_assembler.py with SPARK implementation",
          "spec": "tools/spark/src/core/spec_assembler.ads",
          "body": "tools/spark/src/core/spec_assembler.adb",
          "main": "tools/spark/src/core/spec_assembler_main.adb",
          "functions": [
            {
              "name": "Parse_Extraction_JSON",
              "contract": "Parse extraction JSON without raising exceptions",
              "pre": "Input_File exists and is readable",
              "post": "Returns valid Spec_Input_Data or error code"
            },
            {
              "name": "Normalize_Extraction",
              "contract": "Transform multiple formats into unified spec",
              "pre": "Parsed extraction data",
              "post": "Spec input data conforms to unified schema"
            },
            {
              "name": "Validate_Spec",
              "contract": "Validate spec against schema",
              "pre": "Spec data is populated",
              "post": "All required fields present and valid"
            },
            {
              "name": "Generate_Spec_JSON",
              "contract": "Output valid spec JSON",
              "pre": "Validated spec data",
              "post": "Output file contains valid JSON"
            }
          ]
        },
        {
          "id": "2.2",
          "name": "SPARK IR_Converter",
          "status": "pending",
          "description": "Replace Python ir_converter_bridge.py with SPARK implementation",
          "inputs": ["spec.json"],
          "outputs": ["ir.json", "diagnostics.json"],
          "acceptance_criteria": [
            "IR output validates against ir_schema.json",
            "All functions preserved with correct signatures",
            "IR conversion errors are reported deterministically"
          ],
          "verification_artifacts": ["ir_converter.gpr", "ir_converter.vcg"],
          "files": {
            "spec": "tools/spark/src/core/ir_converter.ads",
            "body": "tools/spark/src/core/ir_converter.adb",
            "main": "tools/spark/src/core/ir_converter_main.adb"
          },
          "functions": [
            {
              "name": "Parse_Spec_JSON",
              "contract": "Parse spec JSON into typed structure",
              "pre": "Input file exists and contains valid JSON",
              "post": "Returns structured spec data or error"
            },
            {
              "name": "Validate_Spec",
              "contract": "Validate spec semantic rules",
              "pre": "Parsed spec data",
              "post": "All function and parameter types are valid"
            },
            {
              "name": "Convert_Spec_To_IR",
              "contract": "Transform spec to intermediate representation",
              "pre": "Valid spec data",
              "post": "IR contains all functions with proper types and steps"
            },
            {
              "name": "Generate_IR_JSON",
              "contract": "Output IR as JSON",
              "pre": "Valid IR data",
              "post": "Output file contains valid IR JSON"
            }
          ]
        },
        {
          "id": "2.3",
          "name": "SPARK Code_Emitter",
          "status": "pending",
          "description": "Replace Python code_emitter_bridge.py with SPARK implementation",
          "inputs": ["ir.json", "target", "output directory"],
          "outputs": ["target source file", "diagnostics.json"],
          "acceptance_criteria": [
            "All 10 targets compile for gnu_bc",
            "No syntax errors in generated code",
            "Default bodies include valid return statements"
          ],
          "verification_artifacts": ["code_emitter.gpr", "code_emitter.vcg"],
          "files": {
            "spec": "tools/spark/src/core/code_emitter.ads",
            "body": "tools/spark/src/core/code_emitter.adb",
            "main": "tools/spark/src/core/code_emitter_main.adb"
          },
          "supported_targets": ["cpp", "python", "rust", "go", "java", "csharp", "javascript", "typescript", "swift", "kotlin"],
          "functions": [
            {
              "name": "Parse_IR_JSON",
              "contract": "Parse IR JSON into typed structure",
              "pre": "Input file exists and contains valid IR JSON",
              "post": "Returns structured IR data or error"
            },
            {
              "name": "Map_Type_To_Target",
              "contract": "Map IR types to target language types",
              "pre": "IR_Type is valid, Target is supported",
              "post": "Returns valid target language type string"
            },
            {
              "name": "Generate_Function_Code",
              "contract": "Generate syntactically correct function",
              "pre": "Valid function IR, supported target",
              "post": "Output compiles in target language"
            },
            {
              "name": "Generate_Header",
              "contract": "Generate file header with imports",
              "pre": "Target language specified",
              "post": "Header is syntactically correct"
            },
            {
              "name": "Generate_Footer",
              "contract": "Generate file footer",
              "pre": "Target language specified",
              "post": "Footer is syntactically correct"
            },
            {
              "name": "Generate_Diagnostics",
              "contract": "Output diagnostics JSON",
              "pre": "Any phase error or warning",
              "post": "Diagnostics are valid and include phase and code"
            }
          ]
        },
        {
          "id": "2.4",
          "name": "SPARK Pipeline_Driver",
          "status": "pending",
          "description": "Orchestrate full pipeline with SPARK contracts",
          "inputs": ["extraction_result.json", "targets", "output directory"],
          "outputs": ["spec.json", "ir.json", "target source files", "diagnostics.json"],
          "acceptance_criteria": [
            "Pipeline runs without unhandled exceptions",
            "Single-phase runs produce the same output as full pipeline",
            "All outputs are deterministic"
          ],
          "verification_artifacts": ["pipeline_driver.gpr", "pipeline_driver.vcg"],
          "files": {
            "spec": "tools/spark/src/core/pipeline_driver.ads",
            "body": "tools/spark/src/core/pipeline_driver.adb",
            "main": "tools/spark/src/core/pipeline_driver_main.adb"
          },
          "functions": [
            {
              "name": "Run_Pipeline",
              "contract": "Execute full pipeline without runtime errors",
              "pre": "Input file exists, output directory writable, target valid",
              "post": "Output files generated or error reported"
            },
            {
              "name": "Run_Phase",
              "contract": "Execute single pipeline phase",
              "pre": "Phase input valid",
              "post": "Phase output valid or error propagated"
            },
            {
              "name": "Emit_Diagnostics",
              "contract": "Emit diagnostics on failure",
              "pre": "Error condition detected",
              "post": "Diagnostics include phase, error code, and message"
            }
          ]
        }
      ]
    },
    {
      "phase": 3,
      "name": "Universal Pipeline Support",
      "priority": "High",
      "description": "Handle different extraction formats from bc, lua, sqlite",
      "tasks": [
        {
          "id": "3.1",
          "name": "Unified Extraction Schema",
          "status": "pending",
          "description": "Define common schema for all extraction formats",
          "schema_file": "tools/spark/schema/extraction_schema.json",
          "required_fields": [
            "module_name",
            "functions",
            "function.name",
            "function.return_type",
            "function.parameters"
          ],
          "optional_fields": [
            "source_files",
            "types",
            "globals",
            "dependencies"
          ]
        },
        {
          "id": "3.2",
          "name": "Multi-Format Parser",
          "status": "pending",
          "description": "Parse different extraction formats into unified schema",
          "formats_supported": [
            {
              "name": "bc_format",
              "characteristics": "Has source_files array, structured functions",
              "example": "gnu_bc extraction"
            },
            {
              "name": "lua_format",
              "characteristics": "Missing source_files, different nesting",
              "example": "lua_5.4 extraction"
            },
            {
              "name": "sqlite_format",
              "characteristics": "Similar to lua format",
              "example": "sqlite extraction"
            }
          ],
          "implementation": {
            "file": "tools/spark/src/core/extraction_parser.ads",
            "approach": "Format detection followed by specific parser"
          }
        },
        {
          "id": "3.3",
          "name": "Schema Validator",
          "status": "pending",
          "description": "SPARK-based JSON schema validation",
          "file": "tools/spark/src/core/schema_validator.ads",
          "functions": [
            {
              "name": "Validate_Extraction",
              "contract": "Validate extraction against unified schema",
              "pre": "JSON is valid and parseable",
              "post": "Returns validation result with error details"
            }
          ]
        }
      ]
    },
    {
      "phase": 4,
      "name": "Verification and Testing",
      "priority": "High",
      "description": "Comprehensive verification and testing suite",
      "tasks": [
        {
          "id": "4.1",
          "name": "SPARK Verification",
          "status": "pending",
          "description": "Prove all SPARK components with SPARK Pro",
          "verification_levels": {
            "stone": "Basic syntax and semantic checks",
            "bronze": "Initialization and basic flow analysis",
            "silver": "Absence of run-time errors (AoRTE)",
            "gold": "Functional correctness of key properties",
            "platinum": "Full functional correctness"
          },
          "target_level": "Gold",
          "deliverables": [
            "tools/spark/reports/verification_report.html",
            "tools/spark/reports/proof_summary.txt",
            "tools/spark/reports/vcg_stats.json"
          ]
        },
        {
          "id": "4.2",
          "name": "Integration Testing",
          "status": "pending",
          "description": "End-to-end pipeline testing",
          "test_programs": ["gnu_bc", "lua_5.4", "sqlite"],
          "test_targets": ["cpp", "python", "rust", "go", "java", "csharp", "javascript", "typescript", "swift", "kotlin"],
          "test_matrix": {
            "compilation": "All generated code must compile",
            "syntax": "All generated code must be syntactically valid",
            "structure": "Generated functions must match input signatures"
          }
        },
        {
          "id": "4.3",
          "name": "Regression Testing",
          "status": "pending",
          "description": "Ensure fixes don't break existing functionality",
          "test_file": "tools/spark/tests/test_regression.py",
          "coverage": ["Type mapping", "Parameter generation", "Return statements", "All target languages"]
        }
      ]
    }
  ],

  "spark_proof_strategy": {
    "flow_analysis": {
      "mode": "auto",
      "objectives": ["Information Flow Security", "Data Initialization", "Global Variable Usage"],
      "tools": ["GNATprove"]
    },
    "runtime_checks": {
      "overflow": "Eliminate constraints errors via range types",
      "division_by_zero": "Preconditions on divisors",
      "index_check": "Loop invariants and array index subtypes",
      "range_check": "Strong typing with static bounds"
    },
    "bounded_execution": {
      "loops": "Must have variant clauses or static iteration bounds",
      "recursion": "Prohibited (except in specific proven specialized routines)",
      "memory": "No dynamic allocation (access types restricted)"
    },
    "contracts": {
      "preconditions": "Define valid input states",
      "postconditions": "Define expected output states and invariants",
      "global_contracts": "Explicitly state all global reads/writes",
      "depends_contracts": "Define information flow dependencies"
    }
  },

  "security_considerations": {
    "input_validation": {
      "strategy": "Fail-fast on any schema violation",
      "sanitization": "Strings bounded to safe lengths, special chars escaped",
      "parsing": "Recursive descent with resource limits (stack depth check)"
    },
    "resource_exhaustion": {
      "mitigation": "Bounded loops, fixed-size buffers, no dynamic memory"
    },
    "determinism": {
      "requirement": "Output must be bit-for-bit identical for same input",
      "implementation": "Sorted map iteration, fixed seeds if random needed"
    }
  },

  "diagnostics": {
    "file": "diagnostics.json",
    "schema": {
      "required": ["phase", "code", "message", "location", "severity", "timestamp"],
      "location": ["file", "line", "column"],
      "optional": ["details", "hint", "target", "function"]
    },
    "codes": [
      "E_PARSE", "E_SCHEMA", "E_VALIDATE", "E_CONVERT", "E_EMIT",
      "E_IO", "E_TARGET", "E_UNSUPPORTED", "W_DEPRECATED", "W_OPTIMIZATION"
    ]
  },

  "compatibility_matrix": {
    "targets": {
      "cpp": {"min_standard": "c++17", "compiler": "g++"},
      "python": {"min_version": "3.10", "compiler": "py_compile"},
      "rust": {"min_version": "1.70", "compiler": "rustc"},
      "go": {"min_version": "1.20", "compiler": "go build"},
      "java": {"min_version": "17", "compiler": "javac"},
      "csharp": {"min_version": "10", "compiler": "csc"},
      "javascript": {"min_version": "ES2020", "compiler": "node --check"},
      "typescript": {"min_version": "5.0", "compiler": "tsc --noEmit"},
      "swift": {"min_version": "5.8", "compiler": "swiftc"},
      "kotlin": {"min_version": "1.8", "compiler": "kotlinc"}
    },
    "spark": {
      "gnat": "2023 or later",
      "spark_pro": "2023 or later",
      "os": ["windows", "linux"]
    }
  },

  "build_and_ci": {
    "build_steps": [
      "gnatmake -P tools/spark/core.gpr",
      "sparkprove -P tools/spark/core.gpr --level=4 --report=all"
    ],
    "test_steps": [
      "python test_stunir_pipeline.py",
      "tools/spark/tests/test_pipeline.adb"
    ],
    "artifacts": [
      "spec.json",
      "ir.json",
      "diagnostics.json",
      "verification_report.html",
      "gnatprove.out"
    ]
  },

  "traceability": {
    "requirements_to_tests": "tests map to phases and acceptance criteria IDs",
    "requirements_to_proofs": "proofs map to SPARK contracts and VCG IDs",
    "evidence_pack": [
      "verification_report.html",
      "proof_summary.txt",
      "vcg_stats.json",
      "test_results.json"
    ]
  },

  "risks_and_mitigations": [
    {
      "risk": "Missing GNAT/SPARK toolchain on Windows",
      "impact": "Unable to build or verify locally",
      "mitigation": "CI builds on Linux with GNAT and SPARK installed; Use Docker container local"
    },
    {
      "risk": "Extraction formats change",
      "impact": "Pipeline failures in Spec_Assembler",
      "mitigation": "Versioned schema with backward-compatible parser"
    },
    {
      "risk": "Combinatorial explosion in path analysis",
      "impact": "Proof timeout",
      "mitigation": "Modularize functions, use ghost code for intermediate assertions"
    }
  ],

  "governance": {
    "change_control": "Pull requests require passed verification and regression tests",
    "versioning": "Semantic versioning for all tools and schemas",
    "review_process": "Code owner review + automated SPARK proof check"
  },

  "file_structure": {
    "tools/spark/": {
      "src/core/": [
        "spec_assembler.ads/adb",
        "spec_assembler_main.adb",
        "ir_converter.ads/adb",
        "ir_converter_main.adb",
        "code_emitter.ads/adb",
        "code_emitter_main.adb",
        "pipeline_driver.ads/adb",
        "pipeline_driver_main.adb",
        "type_mapper.ads/adb",
        "extraction_parser.ads/adb",
        "schema_validator.ads/adb"
      ],
      "src/": [
        "stunir_types.ads/adb",
        "stunir_json_parser.ads/adb"
      ],
      "schema/": [
        "extraction_schema.json",
        "spec_schema.json",
        "ir_schema.json"
      ],
      "tests/": [
        "test_spec_assembler.adb",
        "test_ir_converter.adb",
        "test_code_emitter.adb",
        "test_pipeline.adb",
        "test_type_mapper.adb"
      ],
      "reports/": [
        "verification_report.html",
        "proof_summary.txt",
        "vcg_stats.json"
      ],
      "Makefile": "Build configuration",
      "core.gpr": "GNAT project file",
      "README.md": "Documentation"
    }
  },
  
  "success_criteria": {
    "code_generation": "All 10 target languages produce compilable code",
    "spark_verification": "Gold level achieved for all components",
    "pipeline_coverage": "Works with bc, lua, and sqlite extractions",
    "test_pass_rate": "100% of tests pass",
    "do_333_compliance": "Formal verification evidence package complete"
  }
}
