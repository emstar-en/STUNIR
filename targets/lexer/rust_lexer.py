"""
Rust Lexer Emitter for STUNIR.

Generates Rust lexer code from DFA specification.
"""

from typing import List

from ir.lexer.token_spec import LexerSpec, TokenSpec
from ir.lexer.dfa import MinimizedDFA, TransitionTable
from .base import BaseLexerEmitter, EmitterUtils


class RustLexerEmitter(BaseLexerEmitter):
    """
    Emit Rust lexer code.
    
    Generates a self-contained Rust module with:
    - Token struct
    - TokenType enum
    - DFA transition tables
    - Lexer struct with tokenize() method
    """
    
    def emit(self, spec: LexerSpec, dfa: MinimizedDFA, table: TransitionTable) -> str:
        """Emit complete Rust lexer module."""
        lines = [
            '//!',
            f'//! Generated Lexer: {spec.name}',
            f'//! Tokens: {len(spec.tokens)}',
            f'//! DFA States: {dfa.num_states}',
            f'//! Alphabet Size: {len(dfa.alphabet)}',
            '//!',
            '//! Generated by STUNIR Lexer Generator',
            '//!',
            '',
            'use std::collections::HashMap;',
            'use std::fmt;',
            '',
            self.emit_token_class(spec),
            '',
            self.emit_transition_table(table),
            '',
            self.emit_lexer_class(spec, dfa),
        ]
        return '\n'.join(lines)
    
    def emit_token_class(self, spec: LexerSpec) -> str:
        """Emit Token struct and TokenType enum."""
        lines = [
            '/// Token types',
            '#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]',
            'pub enum TokenType {',
        ]
        
        for token in spec.tokens:
            lines.append(f'    {token.name},')
        
        lines.extend([
            '}',
            '',
            'impl fmt::Display for TokenType {',
            '    fn fmt(&self, f: &mut fmt::Formatter<\'_>) -> fmt::Result {',
            '        match self {',
        ])
        
        for token in spec.tokens:
            lines.append(f'            TokenType::{token.name} => write!(f, "{token.name}"),')
        
        lines.extend([
            '        }',
            '    }',
            '}',
            '',
            '/// Token produced by lexer',
            '#[derive(Debug, Clone)]',
            'pub struct Token {',
            '    pub token_type: TokenType,',
            '    pub value: String,',
            '    pub line: usize,',
            '    pub column: usize,',
            '}',
            '',
            'impl Token {',
            '    pub fn new(token_type: TokenType, value: String, line: usize, column: usize) -> Self {',
            '        Token { token_type, value, line, column }',
            '    }',
            '}',
            '',
            '/// Lexer error',
            '#[derive(Debug)]',
            'pub struct LexerError {',
            '    pub message: String,',
            '    pub line: usize,',
            '    pub column: usize,',
            '}',
            '',
            'impl fmt::Display for LexerError {',
            '    fn fmt(&self, f: &mut fmt::Formatter<\'_>) -> fmt::Result {',
            '        write!(f, "{} at line {}, column {}", self.message, self.line, self.column)',
            '    }',
            '}',
            '',
            'impl std::error::Error for LexerError {}',
        ])
        
        return '\n'.join(lines)
    
    def emit_transition_table(self, table: TransitionTable) -> str:
        """Emit DFA transition table."""
        lines = [
            '// DFA Transition Table Constants',
            f'const START_STATE: i32 = {table.start_state};',
            f'const ERROR_STATE: i32 = {table.error_state};',
            f'const NUM_STATES: usize = {table.num_states};',
            f'const NUM_SYMBOLS: usize = {table.num_symbols};',
            '',
        ]
        
        # Build symbol to index mapping initialization code
        lines.append('/// Build symbol to index mapping')
        lines.append('fn build_symbol_map() -> HashMap<char, usize> {')
        lines.append('    let mut map = HashMap::new();')
        
        for symbol, idx in sorted(table.symbol_to_index.items(), key=lambda x: x[1]):
            escaped = self._escape_rust_char(symbol)
            lines.append(f"    map.insert('{escaped}', {idx});")
        
        lines.append('    map')
        lines.append('}')
        lines.append('')
        
        # Transition table
        lines.append('/// DFA transition table')
        lines.append('static TRANSITIONS: &[i32] = &[')
        for i in range(0, len(table.table), 16):
            chunk = table.table[i:i+16]
            lines.append(f'    {", ".join(str(v) for v in chunk)},')
        lines.append('];')
        lines.append('')
        
        # Accept table - build as function to return Option<(TokenType, i32)>
        lines.append('/// Get accept info for state')
        lines.append('fn get_accept(state: usize) -> Option<(TokenType, i32)> {')
        lines.append('    match state {')
        
        for state_idx, accept_info in enumerate(table.accept_table):
            if accept_info:
                token_name, priority = accept_info
                lines.append(f'        {state_idx} => Some((TokenType::{token_name}, {priority})),')
        
        lines.append('        _ => None,')
        lines.append('    }')
        lines.append('}')
        
        return '\n'.join(lines)
    
    def emit_lexer_class(self, spec: LexerSpec, dfa: MinimizedDFA) -> str:
        """Emit lexer struct and implementation."""
        skip_tokens = [t.name for t in spec.tokens if t.skip]
        skip_check = ' || '.join(f'matches!(token.token_type, TokenType::{t})' for t in skip_tokens) if skip_tokens else 'false'
        
        return f'''/// DFA-based lexer for {spec.name}
pub struct {spec.name}Lexer {{
    input: Vec<char>,
    pos: usize,
    line: usize,
    column: usize,
    symbol_map: HashMap<char, usize>,
}}

impl {spec.name}Lexer {{
    /// Create new lexer from input string
    pub fn new(input: &str) -> Self {{
        {spec.name}Lexer {{
            input: input.chars().collect(),
            pos: 0,
            line: 1,
            column: 1,
            symbol_map: build_symbol_map(),
        }}
    }}
    
    /// Tokenize the entire input
    pub fn tokenize(&mut self) -> Result<Vec<Token>, LexerError> {{
        let mut tokens = Vec::new();
        while self.pos < self.input.len() {{
            let token = self.next_token()?;
            if !Self::is_skip_token(&token) {{
                tokens.push(token);
            }}
        }}
        Ok(tokens)
    }}
    
    /// Tokenize including skip tokens
    pub fn tokenize_all(&mut self) -> Result<Vec<Token>, LexerError> {{
        let mut tokens = Vec::new();
        while self.pos < self.input.len() {{
            tokens.push(self.next_token()?);
        }}
        Ok(tokens)
    }}
    
    /// Check if token should be skipped
    fn is_skip_token(token: &Token) -> bool {{
        {skip_check}
    }}
    
    /// Get next token using longest match
    fn next_token(&mut self) -> Result<Token, LexerError> {{
        if self.pos >= self.input.len() {{
            return Err(LexerError {{
                message: "Unexpected end of input".to_string(),
                line: self.line,
                column: self.column,
            }});
        }}
        
        let mut state = START_STATE;
        let mut last_accept_pos: Option<usize> = None;
        let mut last_accept_info: Option<(TokenType, i32)> = None;
        let mut current_pos = self.pos;
        
        while current_pos < self.input.len() {{
            let ch = self.input[current_pos];
            let idx = match self.symbol_map.get(&ch) {{
                Some(&i) => i,
                None => break,
            }};
            
            if state < 0 {{
                break;
            }}
            
            let next_state = TRANSITIONS[state as usize * NUM_SYMBOLS + idx];
            if next_state == ERROR_STATE {{
                break;
            }}
            
            state = next_state;
            current_pos += 1;
            
            if state >= 0 {{
                if let Some(accept_info) = get_accept(state as usize) {{
                    last_accept_pos = Some(current_pos);
                    last_accept_info = Some(accept_info);
                }}
            }}
        }}
        
        match (last_accept_pos, last_accept_info) {{
            (Some(accept_pos), Some((token_type, _))) => {{
                let lexeme: String = self.input[self.pos..accept_pos].iter().collect();
                let token = Token::new(
                    token_type,
                    lexeme.clone(),
                    self.line,
                    self.column,
                );
                
                // Update position
                for c in lexeme.chars() {{
                    if c == '\\n' {{
                        self.line += 1;
                        self.column = 1;
                    }} else {{
                        self.column += 1;
                    }}
                }}
                self.pos = accept_pos;
                
                Ok(token)
            }}
            _ => {{
                Err(LexerError {{
                    message: format!("Unexpected character '{{}}'" , self.input[self.pos]),
                    line: self.line,
                    column: self.column,
                }})
            }}
        }}
    }}
    
    /// Reset lexer to beginning
    pub fn reset(&mut self) {{
        self.pos = 0;
        self.line = 1;
        self.column = 1;
    }}
}}

/// Convenience function to tokenize a string
pub fn tokenize(input: &str) -> Result<Vec<Token>, LexerError> {{
    let mut lexer = {spec.name}Lexer::new(input);
    lexer.tokenize()
}}

#[cfg(test)]
mod tests {{
    use super::*;
    
    #[test]
    fn test_lexer_creation() {{
        let lexer = {spec.name}Lexer::new("test input");
        assert_eq!(lexer.pos, 0);
        assert_eq!(lexer.line, 1);
    }}
}}
'''
    
    def _escape_rust_char(self, c: str) -> str:
        """Escape character for Rust char literal."""
        if c == '\\':
            return '\\\\'
        elif c == "'":
            return "\\'"
        elif c == '\n':
            return '\\n'
        elif c == '\r':
            return '\\r'
        elif c == '\t':
            return '\\t'
        elif ord(c) < 32 or ord(c) > 126:
            return f'\\u{{{ord(c):04x}}}'
        return c
